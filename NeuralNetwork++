from turtle import width
import numpy as np
import pickle
import matplotlib.pyplot as plt
import matplotlib
font = {'weight' : 'normal','size'   : 22}
matplotlib.rc('font', **font)
import logging
logging.basicConfig(
    format='%(asctime)s %(levelname)-8s %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S')

np.random.seed(102)

logEachNode = False # very slow to log, print every back/forward call
trainPrintMod = 100 # always print this%currentepoch.  We also always print the new best and other conditions.

numStability = 1e-6 # aka epsilon

# here be the magic numbers

hp_max_epochs = 1000
hp_batch_size = 200
hp_stopWhenTrainAccIs100 = True
hp_step_size = 0.001
hp_number_of_layers = 4
hp_width_of_layers = 256
hp_decay = 0.9
hp_opti_method = "ADAM"

    
b1 = 0.9
b2 = 0.999
    

auto_train_number_of_layers = [2,4,8,16,32,64,128,256]
auto_train_width_of_layers = [2,4,8,16,32,64,128,256,512,1024]



def main():

  # Load data
    X_train, Y_train, X_val, Y_val, X_test, Y_test = loadCIFAR10Data()
  
  # Some helpful dimensions
    num_examples, input_dim = X_train.shape
    output_dim = 3 # number of class labels
    
    # Run
    neuralNetwork, acc = trainNeuralNetwork(X_train, Y_train, X_val, Y_val)
  
    auto_train(X_train, Y_train, X_val, Y_val, auto_train_number_of_layers, auto_train_width_of_layers)


  # For each epoch below max epochs

    # Scramble order of examples

    # for each batch in data:

      # Gather batch

      # Compute forward pass

      # Compute loss

      # Backward loss and networks

      # Take optimizer step

      # Book-keeping for loss / accuracy
  
    # Evaluate performance on validation.




def trainNeuralNetwork(X_train, Y_train, X_val, Y_val):
    global hp_step_size
    net = FeedForwardNeuralNetwork(X_train.shape[1], 10, hp_width_of_layers, hp_number_of_layers)
    
    # loss vars
    losses = []; val_losses = []; accs = []; val_accs = []
    
    # rollback vars
    best_net = net; best_net_acc = 0
    
    lossFunc = CrossEntropySoftmax()
    
    inds = np.arange(len(X_train))
    for i in range(1,hp_max_epochs):
      #print("Epoch: " + str(i) + "/" + str(len(X_train)))
      np.random.shuffle(inds)
    
      j = 0
      acc_running = loss_running = 0
      while j < len(X_train):
        #print("Len: " + str(j) + "/" + str(len(X_train)))
        b = min(hp_batch_size, len(X_train)-j)
        X_batch = X_train[inds[j:j+b]]
        Y_batch = Y_train[inds[j:j+b]].astype(int)
      
        
        logits =      net.forward(X_batch)
        loss =        lossFunc.forward(logits, Y_batch)
        predictions = np.argmax(logits,axis=1)[:,np.newaxis]
        acc =         np.mean( predictions == Y_batch)
        loss_grad =   lossFunc.backward()
        
        net.backward(loss_grad)
        net.step(i, hp_step_size*hp_decay**i)
        losses.append(loss)
        accs.append(acc)
        
        loss_running  += loss*b
        acc_running   += acc*b
    
        j+=hp_batch_size
    
      val_loss, val_acc, val_loss_running, val_acc_running = evaluateValidation(net, X_val, Y_val)
      val_losses.append(val_loss)
      val_accs.append(val_acc)
      
      trainAcc = acc_running / len(X_train)*100
      
      loss = loss_running/len(X_train)
      
      
      # keep track of the epoch with the greatest accuracy
      if (val_acc > best_net_acc):
          best_net = net
          best_net_acc = val_acc
          best_trainAcc = trainAcc
          best_loss = loss
          best_i = i
          print("[Epoch {:3}]   Loss:  {:8.4}     Train Acc:  {:8.4}%      Val Acc:  {:8.4}%".format(i, loss, trainAcc, val_acc*100))
         
      # print a line for each of the following conditions: 
          # new best val acc epoch
          # mod from global variable
          # final selected epoch
      elif trainAcc == 100:
          print("[Epoch {:3}]   Loss:  {:8.4}     Train Acc:  {:8.4}%      Val Acc:  {:8.4}%".format(i, loss, trainAcc, val_acc*100))
          if hp_stopWhenTrainAccIs100 == True:
             break
      elif (i%trainPrintMod == 0):
          print("[Epoch {:3}]   Loss:  {:8.4}     Train Acc:  {:8.4}%      Val Acc:  {:8.4}%".format(i, loss, trainAcc, val_acc*100))
        
    print("[Best Epoch {:3}]   Loss:  {:8.4}     Train Acc:  {:8.4}%      Val Acc:  {:8.4}%".format(best_i, best_loss, best_trainAcc, best_net_acc*100))
         
    graphResults(getHyperParamString(), X_train, hp_batch_size, val_losses, val_accs, losses, accs)
    return best_net, best_net_acc*100

def auto_train(X_train, Y_train, X_val, Y_val, train_number_of_layers, train_width_of_layers):
    
    global hp_number_of_layers
    global hp_width_of_layers
    
    print("Auto_Train commenced!  Start your engines.")

    best_acc = 0
    best_nn = []
    for n in train_number_of_layers:
        print("")
        for w in train_width_of_layers:
            printDiv(1)
            print("Training -  hp_number_of_layers: " + str(n) + " hp_width_of_layers: " + str(w))
            hp_number_of_layers = n
            hp_width_of_layers = w
            neuralNetwork, acc = trainNeuralNetwork(X_train, Y_train, X_val, Y_val)
            if (acc > best_acc):
                best_nn = neuralNetwork
                best_acc = acc
                best_number_of_layers = hp_number_of_layers
                best_width_of_layers = hp_width_of_layers
                printDiv(2)
                print("Found new best -  ")
                print(getHyperParamString())
                printDiv(2)
    hp_number_of_layers = best_number_of_layers
    hp_width_of_layers = best_width_of_layers
    return


class CrossEntropySoftmax:
  
  def forward(self, logits, labels):
    if logEachNode == True:
        print("[softmax forward]")
    self.probs = softmax(logits)
    self.labels = labels
    ret = -np.mean(np.log(self.probs[np.arange(len(self.probs))[:,np.newaxis],labels]+numStability))
    return ret

  def backward(self):
    if logEachNode == True:
        print("[softmax backward]")
    grad = self.probs
    grad[np.arange(len(self.probs))[:,np.newaxis],self.labels] -= 1
    ret = grad.astype(np.float64)/len(self.probs)
    return ret


class ReLU:

  def forward(self, input):
    if logEachNode == True:
        print("[ReLU forward]")
    self.mask = (input > 0)
    ret = input * self.mask
    return ret
      
  def backward(self, grad):
    if logEachNode == True:
        print("[ReLU backward]")
    ret = grad * self.mask
    return ret

  def step(self, stepNum, step_size):
    if logEachNode == True:
        print("[ReLU step]")
    return


class LinearLayer:

  def __init__(self, input_dim, output_dim):
    self.weights = np.random.randn(input_dim, output_dim)* np.sqrt(2. / input_dim)
    self.bias = np.ones((1,output_dim))*0.5
    self.s = np.zeros((1,output_dim))
    self.r = np.zeros((1,output_dim))
    
  def forward(self, input):
    if logEachNode == True:
        print("[LinearLayer forward]")
    self.input = input #Storing X
    ret = self.input @ self.weights + self.bias
    return ret

  def backward(self, grad):
    if logEachNode == True:
        print("[LinearLayer backward]")
    self.grad_weights = self.input.T @ grad
    self.grad_bias = grad.sum()
    ret = grad @ self.weights.T
    return ret
    
  def step(self, currentStep, step_size):
    if logEachNode == True:
        print("[LinearLayer step]")
    if hp_opti_method == "SGD":
        self.weights -= hp_step_size*self.grad_weights
        self.bias -= hp_step_size*self.grad_bias
    if hp_opti_method == "ADAM":
        
        self.s = b1 * self.s + (1 - b1) * self.grad_weights
        self.r = b2 * self.r + (1 - b2) * self.grad_weights ** 2
        s_hat = self.s / (1 - b1 ** (hp_step_size+1))
        r_hat = self.r / (1 - b2 ** (hp_step_size+1))
        
        self.weights = self.weights - hp_step_size * (s_hat / (np.sqrt(r_hat) + numStability))
        self.bias -= hp_step_size*self.grad_bias
        
        
        """
        m = [0 for i in range(len(self.weights))]
        v = [0 for i in range(len(self.weights))]
        g = [0 for i in range(len(self.weights))]
        
        # m = mean
        # v = variance
        
        g = self.grad_weights
        
        # Update the m and v parameter
        m = [b1*m_i + (1 - b1)*g_i for m_i, g_i in zip(m, g)]
        v = [b2*v_i + (1 - b2)*(g_i**2) for v_i, g_i in zip(v, g)]
        
        # Bias correction for m and v
        m_cor = [m_i / (1 - (b1**stepNum)) for m_i in m]
        v_cor = [v_i / (1 - (b2**stepNum)) for v_i in v]
        
        
        # # Update the parameter
        new_g = [weight - (learning_rate / (np.sqrt(v_cor_i) + epsilon))*m_cor_i for weight, v_cor_i, m_cor_i in zip(g, v_cor, m_cor)]
        new_g = np.asarray(new_g)
        diff = hp_step_size*new_g
        self.grad_weights -= hp_step_size*new_g
        self.bias -= hp_step_size*self.grad_bias
        """
        


def evaluateValidation(model, X_val, Y_val):
  val_loss_running = []
  val_acc_running = []
  j=0

  lossFunc = CrossEntropySoftmax()

  while j < len(X_val):
    b = min(hp_batch_size, len(X_val)-j)
    X_batch = X_val[j:j+b]
    Y_batch = Y_val[j:j+b].astype(int)
   
    logits = model.forward(X_batch)
    loss = lossFunc.forward(logits, Y_batch)
    predictions = np.argmax(logits,axis=1)[:,np.newaxis]
    acc = np.mean( predictions == Y_batch)

    val_loss_running.append(loss)
    val_acc_running.append(acc)
       
    j+=hp_batch_size

  val_loss = np.mean(val_loss_running)
  val_acc =  np.mean(val_acc_running)
  return val_loss, val_acc, val_loss_running, val_acc_running


def softmax(x):
  ret_x = x - np.max(x,axis=1)[:,np.newaxis]  # Numerical stability trick
  ret = np.exp(ret_x) / (np.sum(np.exp(ret_x),axis=1)[:,np.newaxis]) 
  return ret


class FeedForwardNeuralNetwork:

  def __init__(self, input_dim, output_dim, hidden_dim, num_layers):
 
    if num_layers == 1:
      self.layers = [LinearLayer(input_dim, output_dim)]
    else:
      self.layers = [LinearLayer(input_dim, hidden_dim)]
      self.layers.append(ReLU())
      for i in range(num_layers-2):
        self.layers.append(LinearLayer(hidden_dim, hidden_dim))
        self.layers.append(ReLU())
      self.layers.append(LinearLayer(hidden_dim, output_dim))

  def forward(self, X):
    for layer in self.layers:
      X = layer.forward(X)
    return X

  def backward(self, grad):
    for layer in reversed(self.layers):
      grad = layer.backward(grad)

  def step(self, currentStep, step_size):
    for layer in self.layers:
      layer.step(currentStep, step_size)





#####################################################
# Utility Functions for Loading and Visualizing Data
#####################################################

def loadCIFAR10Data(normalize = True):

  with open("cifar10_hst_train", 'rb') as fo:
    data = pickle.load(fo)
  X_train = data['images']
  Y_train = data['labels']

  with open("cifar10_hst_val", 'rb') as fo:
    data = pickle.load(fo)
  X_val = data['images']
  Y_val = data['labels']

  with open("cifar10_hst_test", 'rb') as fo:
    data = pickle.load(fo)
  X_test = data['images']
  Y_test = data['labels']
  
  
  if normalize:
    X_train = X_train/256-0.5
    X_val = X_val/256-0.5
    X_test = X_test/256-0.5
  
  logging.info("Loaded train: " + str(X_train.shape))
  logging.info("Loaded val: " + str(X_val.shape))
  logging.info("Loaded test: " + str(X_test.shape))
  
  return X_train, Y_train, X_val, Y_val, X_test, Y_test



def graphResults(name, X_train, hp_batch_size, val_losses, val_accs, losses, accs):
    
    
    fig, ax1 = plt.subplots(figsize=(16,9))
    fig.suptitle(name)
    color = 'tab:red'
    ax1.plot(range(len(losses)), losses, c=color, alpha=0.25, label="Train Loss")
    ax1.plot([np.ceil((i+1)*len(X_train)/hp_batch_size) for i in range(len(val_losses))], val_losses,c="red", label="Val. Loss")
    ax1.set_xlabel("Iterations")
    ax1.set_ylabel("Avg. Cross-Entropy Loss", c=color)
    ax1.tick_params(axis='y', labelcolor=color)
    ax1.set_ylim(-0.01,3)

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis

    color = 'tab:blue'
    ax2.plot(range(len(losses)), accs, c=color, label="Train Acc.", alpha=0.25)
    ax2.plot([np.ceil((i+1)*len(X_train)/hp_batch_size) for i in range(len(val_accs))], val_accs,c="blue", label="Val. Acc.")
    ax2.set_ylabel(" Accuracy", c=color)
    ax2.tick_params(axis='y', labelcolor=color)
    ax2.set_ylim(-0.01,1.01)

    fig.tight_layout()  # otherwise the right y-label is slightly clipped
    ax1.legend(loc="center")
    ax2.legend(loc="center right")
    plt.show()
    
    return

def printDiv(type=1):
    if type==1:
        print("--------------------------------------------------------------")
    if type==2:
        print("==============================================================")
    return
        
def getHyperParamString():
    ret = ""
    ret += "hp_max_epochs = " + str(hp_max_epochs) + "\n"
    ret += "hp_batch_size = " +  str(hp_batch_size) + "\n"
    ret += "hp_stopWhenTrainAccIs100 = " +  str(hp_stopWhenTrainAccIs100) + "\n"
    ret += "hp_step_size = " +   str(hp_step_size) + "\n"
    ret += "hp_number_of_layers = " + str(hp_number_of_layers) + "\n"
    ret += "hp_width_of_layers = " + str(hp_width_of_layers) + "\n"
    ret += "hp_decay = " + str(hp_decay) + "\n"
    return ret
    

def displayExample(x):
  r = x[:1024].reshape(32,32)
  g = x[1024:2048].reshape(32,32)
  b = x[2048:].reshape(32,32)
  
  plt.imshow(np.stack([r,g,b],axis=2))
  plt.axis('off')
  plt.show()


if __name__=="__main__":
  main()
